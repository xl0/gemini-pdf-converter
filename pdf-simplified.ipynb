{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <div style=\"background-image: linear-gradient(rgba(255, 255, 255, 0.5), rgba(255, 255, 255, 0.5)), url('assets/paper.png')\"> -->\n",
    "\n",
    "## Converting PDF documents to Markdown with Google Gemini\n",
    "\n",
    "<img src=\"assets/is_all_you_need.png\" width=\"400\" style=\"display: inline\" />\n",
    "<img src=\"assets/lowry.png\" width=\"300\" style=\"display: inline\" />\n",
    "\n",
    "\n",
    "### The problem\n",
    "\n",
    "Unlike Madkdown or HTML, the data in a PDF page does not always come in the natural order. \n",
    "It's more like a canvas, with a list of elements like:\n",
    "- `(\"World\", font, location, transforms, ...)`\n",
    "- `(\"Hello\", font, location, transforms, ...)`\n",
    "- `([Image], location, transforms, ...)`\n",
    "\n",
    "that are placed at the correct location during rendering. It's generally not possible to reliably extract text  \n",
    "in the natural order, and it gets especially messy with formulas:\n",
    "\n",
    "![Attention](assets/attention.png)\n",
    "```\n",
    "In practice, we compute the attention function on a set of queries simultaneously, packed together | {...}\n",
    "into a matrix | {...}\n",
    " Q | {...}\n",
    ". The keys and values are also packed together into matrices | {...}\n",
    " K | {...}\n",
    " and | {...}\n",
    " V | {...}\n",
    " . We compute | {...}\n",
    "the matrix of outputs as: | {...}\n",
    "Attention( | {...}\n",
    "Q, K, V | {...}\n",
    " ) = softmax( | {...}\n",
    "QK | {...}\n",
    "T | {...}\n",
    "‚àö | {...}\n",
    "d | {...}\n",
    "k | {...}\n",
    ") | {...}\n",
    "V | {...}\n",
    "(1) | {...}\n",
    "```\n",
    "\n",
    "On top of this, many older PDFs are scanned paper documents, and contain poorly  \n",
    "recognized text, no text at all, full-page images instead of text, or images of page fragments.\n",
    "\n",
    "Especially for a scanned PDF, we need to use AI to correctly extract the text  \n",
    "and image from the document.\n",
    "\n",
    "### Procedure\n",
    "\n",
    "- Extract text and image from the document\n",
    "- ü§ñ Ask Gemini if the images look good.\n",
    "    - If the images look good, we don't need to do the extra extraction step.\n",
    "    - If they don't look good:\n",
    "        - ü§ñ Use Gemini to find bounding boxes for all images in the page.\n",
    "        - Extract images from the bounding boxes.\n",
    " - ü§ñ Give the page image, extracted text, and extracted images to Gemini, and generate markdown with references to images.\n",
    "- Combine the markdown for all pages into a single document.\n",
    "- ü§ñ Pass the full document to Gemini and ask it to brush up the formatting to make it consistent across pages.\n",
    "    - This step requires multi-step generation and context caching.\n",
    "- Save the resulting markdown and images.\n",
    "\n",
    "### UI\n",
    "There us a UI at the end of the notebok. Run the notebook (with `RUN_DEV_CELLS=False` for speed) and use it to convert your PDFs:\n",
    "\n",
    "<img src=\"assets/ui.png\" width=\"400\" />\n",
    "\n",
    "### API key\n",
    "Set `GEMINI_API_KEY=\"your very secret key\"` in the environment\n",
    "\n",
    "### Findings\n",
    "\n",
    "Durig this fun projects I found:\n",
    "- Gemini works best with simple and concise tasks. Initially I tried to combine multiple tasks into one step, and the model was struggling.\n",
    "- Gemini is able to find figures/plots/images in a PDF page, but it's not 100% reliable on its own.\n",
    "- Gemini is excellent at converting between plain text and different text-based formats.\n",
    "\n",
    "#### üö©ü•Ω Recitation error\n",
    "Gemini will fail with finish reason: `RECITATION` that triggers when the model is generating large chunks of  \n",
    "material from some Google's database, regardless of license. This of couse makes using Gemini for format  \n",
    "conversion challenging, as the generated text will often match large chunks of known material.\n",
    "\n",
    "**GOOGLE: Pretty please, allow recitation if the recited material is also fully/largely present in the models input!**\n",
    "\n",
    "Luckily, this mechanism can be fooled easily. I ask the model to insert\n",
    "```\n",
    "[<end of paragraph>]\n",
    "```\n",
    "after every paragraph, and cut them out later. This seems to be enough to avoid this error.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### There are two ways to run this notebook\n",
    "#### `RUN_DEV_CELLS=True` - will test the functions as I did during development, this is rather slow. Enable it to follow the inner works.\n",
    "#### `RUN_DEV_CELLS=False` - runs vert fast and you can use the UI at the end of the notebook to convert your PDFs into markdown!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_DEV_CELLS=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "import fitz\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "from google.generativeai.protos import Candidate\n",
    "from google.generativeai import caching\n",
    "import datetime\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "from time import sleep\n",
    "from dataclasses import dataclass, field\n",
    "import random\n",
    "import string\n",
    "from IPython.display import Markdown\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import json\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I tried with Flash, but the results were significantly worse, especially for finding the bounding boxes.\n",
    "# It might be possible to use Flash for some of the sub-tasks.\n",
    "\n",
    "## You need to specify the model postfix (002) for the cache to work.\n",
    "MODEL_NAME=\"gemini-1.5-pro-002\"\n",
    "\n",
    "# Gemini is a bit trigger-happy on the filters, and some research papers might get flagged.\n",
    "SAFETY_SETTINGS = {\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "\n",
    "# Gemini has another annoying feature - it detects when the genreatio is reciting cipyrighted content,\n",
    "# which you'd expect would happen all the time when converting a PDF to markdown.\n",
    "\n",
    "# Surprisingly, it only triggers infrequently and rather randomly, even with temperature=0,\n",
    "# and does not trigger if I retry the exact same request.\n",
    "\n",
    "# As a workaround, retry this many times if the request fails (likely with a RECITATION reason)\n",
    "# I've seen the requests infrequently fail for other reasons, so we will retry for any errors.\n",
    "MAX_RETRIES=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class PDFImage:\n",
    "    image: Image.Image\n",
    "    name: str\n",
    "    bbox: list[int] = field(default_factory=list)  # For images we extract later, keep the bbox.\n",
    "    export: bool = False                           # Export images will be saved as files.\n",
    "\n",
    "@dataclass\n",
    "class PDFPage:\n",
    "    page_num: int\n",
    "    extracted_text: str;\n",
    "    page_image: Image.Image\n",
    "    extracted_images: list[PDFImage];\n",
    "\n",
    "    # Used to keep track of intermediate steps for debugging.\n",
    "    extracted_images_analyzed: Any = field(default_factory=dict);\n",
    "    images_proposed: list[Any] = field(default_factory=list);\n",
    "\n",
    "    markdown: str = \"\";\n",
    "\n",
    "@dataclass\n",
    "class PDFDocument:\n",
    "    pages: list[PDFPage];\n",
    "    markdown: str= \"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the demo in the description.\n",
    "# pdf = fitz.open(\"AIAYN.pdf\")\n",
    "\n",
    "# texts = []\n",
    "# for block in pdf[3].get_text(\"dict\")[\"blocks\"]:\n",
    "#     if (block[\"type\"] == 0):\n",
    "#         for line in block[\"lines\"]:\n",
    "#             for span in line[\"spans\"]:\n",
    "#                 print(span[\"text\"] + \" | {...}\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use PyMuPDF to extract text and images from a PDF. No AI yet. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_content(pdf_path):\n",
    "    pdf = fitz.open(pdf_path)\n",
    "\n",
    "    pages: list[PDFPage] = []\n",
    "    num_images = 0\n",
    "\n",
    "    for page_num, pdf_page in enumerate(tqdm(pdf, desc=\"Extracting PDF content\", unit=\"page\")):\n",
    "        extracted_images: list[PDFImage] = []\n",
    "\n",
    "        # Get images\n",
    "        image_list = pdf_page.get_images()\n",
    "        for img in image_list:\n",
    "            xref = img[0]\n",
    "            base_image = pdf.extract_image(xref)\n",
    "\n",
    "            extracted_images.append(PDFImage(\n",
    "                image=Image.open(io.BytesIO(base_image[\"image\"])),\n",
    "                name=f\"image_{num_images}.png\"\n",
    "\n",
    "            ))\n",
    "            num_images += 1\n",
    "\n",
    "        pages.append(PDFPage(\n",
    "            page_image=Image.open(io.BytesIO(pdf_page.get_pixmap(dpi=300, annots=False).tobytes())),\n",
    "            extracted_text=pdf_page.get_text(),\n",
    "            extracted_images=extracted_images,\n",
    "            page_num=page_num\n",
    "        ))\n",
    "\n",
    "    return PDFDocument(pages=pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try with a well-formatted PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_DEV_CELLS:\n",
    "    aiayn = extract_pdf_content(\"AIAYN.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pdf_page(pdf_page: PDFPage):\n",
    "    display(pdf_page.page_image.resize((350, 500)))\n",
    "    for image in pdf_page.extracted_images:\n",
    "        img = image.image\n",
    "        width, height = img.size\n",
    "        aspect = width / height\n",
    "        if width > height:\n",
    "            new_width = 200\n",
    "            new_height = int(200 / aspect)\n",
    "        else:\n",
    "            new_height = 200\n",
    "            new_width = int(200 * aspect)\n",
    "        display(img.resize((new_width, new_height)))\n",
    "    print(pdf_page.extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_DEV_CELLS:\n",
    "    show_pdf_page(aiayn.pages[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not bad. We lost formatting, the formulas are messed up, and we don't know where to place the images in the page. We can clean up the text using Gemini, and use the extracted hi-res images as is, with some extra annotations.\n",
    "\n",
    "### Let's try with a scanned document that has worse formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if RUN_DEV_CELLS:\n",
    "    lowry = extract_pdf_content(\"Lowry.pdf\")\n",
    "    show_pdf_page(lowry.pages[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a mess. Not only is the text all over the place, the extracted images are just slices of the page.\n",
    "\n",
    "### We will need to use Gemini to both re-OCR the text, and locate the images in the page.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Handle retries, caching, and errors.\n",
    "\n",
    "def gemini_generate(model,\n",
    "                    messages,\n",
    "                    generation_config=None,\n",
    "                    safety_settings=SAFETY_SETTINGS,\n",
    "                    retries=MAX_RETRIES):\n",
    "\n",
    "    attempts = 0\n",
    "\n",
    "    while attempts < retries:\n",
    "        if (attempts > 0):\n",
    "            warnings.warn(f\"Retry {attempts} of maximum {retries}\", stacklevel=1)\n",
    "\n",
    "        response = model.generate_content(\n",
    "            messages,\n",
    "            generation_config=generation_config,\n",
    "            safety_settings=safety_settings\n",
    "        )\n",
    "\n",
    "        if response.candidates[0].finish_reason == Candidate.FinishReason.STOP:\n",
    "            return response.text\n",
    "\n",
    "        if len(response.candidates) == 0:\n",
    "            warnings.warn(\"No candidates received from the model.\", stacklevel=1)\n",
    "\n",
    "        if not response.candidates[0].finish_reason == Candidate.FinishReason.STOP:\n",
    "            warnings.warn(f\"Unexpected finish reason {response.candidates[0].finish_reason.name}\", stacklevel=1)\n",
    "\n",
    "        attempts += 1\n",
    "\n",
    "    # If we reach here, we have failed all retries.\n",
    "    if len(response.candidates) == 0:\n",
    "        warnings.warn(response, stacklevel=1)\n",
    "        raise Exception(\"No candidates received from the model.\")\n",
    "\n",
    "    if not response.candidates[0].finish_reason == Candidate.FinishReason.STOP:\n",
    "        warnings.warn(response, stacklevel=1)\n",
    "        raise Exception(f\"Unexpected finish reason {response.candidates[0].finish_reason}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_enough_prompt=\"\"\"\n",
    "You are given a page from a PDF document, and images automatically extracted from it.\n",
    "\n",
    "First, check if the original page actually has images (figures, plots, diagrams, etc) in it.\n",
    "If no, return an empty list for the image ratings.\n",
    "If yes, for all provided images, determine if the image is extracted perfectly.\n",
    "\n",
    "A perfectly extracted image fully captures one or more figure, plot, or diagram in the page.\n",
    "\n",
    "Poorly extracted images:\n",
    "- Extracted image captures only part of a figure, plot, or diagram.\n",
    "- Extracted image is an image of the full page with mostly text in it.\n",
    "\n",
    "Be picky.\n",
    "It's ok if all images are good or bad. Briefly explain your reasoning before making a decision.\n",
    "\"\"\"\n",
    "\n",
    "# I found that even for this simple task a bit of COT makes a day and night difference.\n",
    "# The model will generate the values in alphabetical order of the keys, so we need to place reasoning before decision.\n",
    "class ImageRating(TypedDict):\n",
    "    a_name: str\n",
    "    b_reason: str\n",
    "    c_good: bool\n",
    "\n",
    "class NeedManualExtraction(TypedDict):\n",
    "    a_page_has_images: bool\n",
    "    image_ratings: list[ImageRating]\n",
    "\n",
    "\n",
    "def analyze_extracted_images(page: PDFPage):\n",
    "    model = genai.GenerativeModel(model_name=MODEL_NAME)\n",
    "\n",
    "    images = []\n",
    "    for image in page.extracted_images:\n",
    "        images.append(f\"Extracted image '{image.name}':\\n\")\n",
    "        images.append(image.image)\n",
    "\n",
    "    result = gemini_generate(\n",
    "        model,\n",
    "        [\"PDF Page:\\n\", page.page_image, \"Extracted images:\\n\", *images, good_enough_prompt],\n",
    "        generation_config=genai.GenerationConfig(\n",
    "            max_output_tokens=2000,\n",
    "            temperature=0,\n",
    "            top_k=1,\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=NeedManualExtraction,\n",
    "        ),\n",
    "        safety_settings=SAFETY_SETTINGS\n",
    "    )\n",
    "\n",
    "    page.extracted_images_analyzed = json.loads(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_DEV_CELLS:\n",
    "    analyze_extracted_images(aiayn.pages[0])\n",
    "    analyze_extracted_images(aiayn.pages[3])\n",
    "    analyze_extracted_images(lowry.pages[0])\n",
    "    analyze_extracted_images(lowry.pages[1])\n",
    "    analyze_extracted_images(lowry.pages[6])\n",
    "\n",
    "    print(\"AIAYN no images: \" + json.dumps(aiayn.pages[0].extracted_images_analyzed, indent=2))\n",
    "    print(\"AIAYN with good images: \" + json.dumps(aiayn.pages[3].extracted_images_analyzed, indent=2))\n",
    "\n",
    "    print(\"Lowry no images: \" + json.dumps(lowry.pages[0].extracted_images_analyzed, indent=2))\n",
    "    print(\"Lowry no images: \" + json.dumps(lowry.pages[1].extracted_images_analyzed, indent=2))\n",
    "    print(\"Lowry bad images: \" + json.dumps(lowry.pages[6].extracted_images_analyzed, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini correctly recognized if a pages has images in it, and for a page with images it correctly recognized if the images are good or not.\n",
    "### Looks promising. I will assume that the page contains good images if the model gives üëç for 80%+ of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_needs_image_extraction(page: PDFPage):\n",
    "    analyze_extracted_images(page)\n",
    "\n",
    "    if not page.extracted_images_analyzed.get(\"a_page_has_images\", False):\n",
    "        # Remove the wrongly extracted images from pages without images to avoid confusing the model later\n",
    "        page.extracted_image = []\n",
    "        return False\n",
    "\n",
    "    good_images = [rating.get(\"c_good\", False) for rating in page.extracted_images_analyzed.get(\"image_ratings\", [])]\n",
    "\n",
    "    # <= to account for the case where there are images, but none have been extracted.\n",
    "    if good_images.count(True) <= 0.8 * len(good_images):\n",
    "        # Images bad overall - remove everything.\n",
    "        page.extracted_images = []\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_DEV_CELLS:\n",
    "    print(page_needs_image_extraction(aiayn.pages[3]))\n",
    "    print(page_needs_image_extraction(lowry.pages[6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, if the page needs AI-asisted image extraction..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think Gemini prefers this order or coordinates.\n",
    "image_extraction_prompt = \"\"\"\n",
    "Annotate the images in the document with a meaningful filename (.png) and a bounding box.\n",
    "Only provide one annotation for each image. Do not annotate tables.\n",
    "\n",
    "JSON schema:\n",
    "[\n",
    "    {\n",
    "        a_caption: \"Image caption\",\n",
    "        bbox: [top, left, bottom, right]\n",
    "    },\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "class ImageAnnotation(TypedDict):\n",
    "    a_caption: str\n",
    "    bbox: list[int]\n",
    "\n",
    "\n",
    "def page_find_bboxes(page: PDFPage):\n",
    "    model = genai.GenerativeModel(model_name=MODEL_NAME)\n",
    "    result = gemini_generate(\n",
    "        model,\n",
    "        [page.page_image, image_extraction_prompt],\n",
    "        generation_config=genai.GenerationConfig(\n",
    "            temperature=0,\n",
    "            top_k=1,\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=list[ImageAnnotation]\n",
    "        ),\n",
    "        safety_settings=SAFETY_SETTINGS\n",
    "    )\n",
    "\n",
    "    page.images_proposed = []\n",
    "\n",
    "    # Sanity check the bounding boxes and unswap coordinates if they seem to have been swapped (happens infrequently)\n",
    "    for image in json.loads(result):\n",
    "        bbox = image.get(\"bbox\")\n",
    "        if len(bbox) == 4:\n",
    "            page.images_proposed.append({\n",
    "                \"a_caption\": image.get(\"a_caption\", \"unknown\"),\n",
    "                \"bbox\": [min(bbox[0], bbox[2]), min(bbox[1], bbox[3]), max(bbox[0], bbox[2]), max(bbox[1], bbox[3])]\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_DEV_CELLS:\n",
    "    page_find_bboxes(lowry.pages[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "def visualize_bboxes(page: PDFPage):\n",
    "    img = page.page_image.copy()\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"cyan\", \"magenta\"]\n",
    "    width, height = img.size\n",
    "\n",
    "    for i, bbox in enumerate([image[\"bbox\"] for image in page.images_proposed]):\n",
    "        # The order of coordintes is different in PIL\n",
    "        bbox_img = [\n",
    "            bbox[1] * width / 1000,   # xmin\n",
    "            bbox[0] * height / 1000,  # ymin\n",
    "            bbox[3] * width / 1000,   # xmax\n",
    "            bbox[2] * height / 1000,  # ymax\n",
    "        ]\n",
    "        draw.rectangle(bbox_img, outline=colors[i % len(colors)], width=5)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_DEV_CELLS:\n",
    "    display(visualize_bboxes(lowry.pages[6]).resize((350, 500)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's not perfect, but might be good enough.\n",
    "\n",
    "> I tried adding more bboxes that are shifted/scaled around the predicted one, extracting the images and asking Gemini to score how well they are cropped, but for some reason Gemini had a very hard time figuring out which ones had the best crop.\n",
    "\n",
    "I will scale up the bbox by 15% and hope it captures the whole image.\n",
    "A dedicated fine-tuned object detection model would give better results, but let's stick to the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_bbox(bbox: list[int], scale):\n",
    "    assert(len(bbox) == 4)\n",
    "\n",
    "    ymin, xmin, ymax, xmax = bbox\n",
    "\n",
    "    width = xmax - xmin\n",
    "    height = ymax - ymin\n",
    "\n",
    "    center_x = (xmin + xmax) / 2\n",
    "    center_y = (ymin + ymax) / 2\n",
    "\n",
    "    scaled_width = width * scale\n",
    "    scaled_height = height * scale\n",
    "\n",
    "    scale_bbox = [\n",
    "        center_y - scaled_height/2, center_x - scaled_width/2,\n",
    "        center_y + scaled_height/2, center_x + scaled_width/2\n",
    "    ]\n",
    "\n",
    "    return [max(0, min(1000, int(x))) for x in scale_bbox]\n",
    "\n",
    "def page_scale_bboxes(page: PDFPage, scale):\n",
    "    for image in page.images_proposed:\n",
    "        image[\"bbox\"] = scale_bbox(image[\"bbox\"], scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_DEV_CELLS:\n",
    "    page_scale_bboxes(lowry.pages[6], 1.15)\n",
    "    display(visualize_bboxes(lowry.pages[6]).resize((350, 500)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We get a bit more text around the image, but I think it's a better alternative to cropping the image.\n",
    "\n",
    "Another solution would be to pass the images with rendered bboxes to Gemini and ask it to adjust the bboxes, but I did not try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds the extracted images to the page.\n",
    "\n",
    "# I will give a random name to extracted images to avoid biasing the model based on the image number.\n",
    "# Store the names in a list to avoid duplicates.\n",
    "random_names = []\n",
    "\n",
    "def extract_images_from_page(page: PDFPage):\n",
    "\n",
    "    # Drop the previouly extracted bad images.\n",
    "    page.extracted_images = []\n",
    "\n",
    "    for bbox in [image[\"bbox\"] for image in page.images_proposed]:\n",
    "        ymin, xmin, ymax, xmax = bbox\n",
    "        width, height = page.page_image.size\n",
    "\n",
    "        bbox_img = page.page_image.crop((\n",
    "            xmin * width / 1000, ymin * height / 1000,\n",
    "            xmax * width / 1000, ymax * height / 1000\n",
    "        ))\n",
    "\n",
    "        while True:\n",
    "            random_name = ''.join(random.choices(string.ascii_lowercase + string.digits, k=5))\n",
    "            if random_name not in random_names:\n",
    "                random_names.append(random_name)\n",
    "                break\n",
    "\n",
    "        page.extracted_images.append(PDFImage(\n",
    "            image=bbox_img,\n",
    "            name=f\"image_{random_name}.png\",\n",
    "            bbox=bbox,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_DEV_CELLS:\n",
    "    extract_images_from_page(lowry.pages[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the anti-reciting hack.\n",
    "def remove_recitation_hack(markdown: str):\n",
    "    return markdown.replace(\"[end of paragraph]\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "common_markdown_prompt = \"\"\"\n",
    "\n",
    "Use markdown heading levels for sections.\n",
    "\n",
    "Insert 2 new lines to force a line bvreak.\n",
    "\n",
    "For images:\n",
    "<img src=\"filename\" width=\"x\">\n",
    "\n",
    "For tables:\n",
    "Use markdown tables, don not include images of tables.\n",
    "\n",
    "For math formulas:\n",
    "Use markdown formulas `$...$ ` for inline formulas, and block formulas \\n$$\\n...\\n$$\\n for block formulas.\n",
    "Use \\\\tag{n} if you need to number a block formula.\n",
    "\n",
    "For chemical formulas:\n",
    "Use <sub> and <sup>\n",
    "\n",
    "For figure captions:\n",
    "Use markdown quotes (> Figure n: caption)\n",
    "\n",
    "Escape symbols in text that would be wrongly interpreted as markdown (#, $, *, etc)\n",
    "\n",
    "Insert [end of paragraph] after each paragraph.\n",
    "\n",
    "Wrap the output in ```markdown.\n",
    "\"\"\"\n",
    "\n",
    "page_markdown_prompt = \"\"\"\n",
    "Convert the page to markdown. Convert all text as is, don't skip any parts, don't change wording.\n",
    "Ignore purely decorative elements.\n",
    "\n",
    "If a page contains figures, plots, diagrams, etc and its image for it is available, include it in markdown.\n",
    "Place each image roughly where it appears in the page. Avoid breaking sentences to fit images.\n",
    "\n",
    "\"\"\" + common_markdown_prompt\n",
    "\n",
    "\n",
    "def page_to_markdown(page: PDFPage):\n",
    "    model = genai.GenerativeModel(model_name=MODEL_NAME)\n",
    "    image_messages = []\n",
    "    for img in page.extracted_images:\n",
    "        image_messages.append(f\"Image variant {img.name}:\\n\")\n",
    "        image_messages.append(img.image)\n",
    "\n",
    "    markdown = gemini_generate(\n",
    "        model,\n",
    "        [\n",
    "            page.page_image,\n",
    "            f\"Extracted page text:\\n{page.extracted_text}\",\n",
    "            \"Extracted images:\",\n",
    "            *image_messages,\n",
    "            page_markdown_prompt,\n",
    "            # Gemini often wraps the output in ```markdown or ```text. I prefill it for consistency.\n",
    "            \"```markdown\\n\", \"model\"\n",
    "        ],\n",
    "        generation_config=genai.GenerationConfig(\n",
    "            temperature=0,\n",
    "            top_k=1,\n",
    "            response_mime_type=\"text/plain\"\n",
    "        ),\n",
    "        safety_settings=SAFETY_SETTINGS\n",
    "    )\n",
    "\n",
    "    prefix = \"```markdown\\n\"\n",
    "    suffix = \"```\"\n",
    "\n",
    "    if markdown.startswith(prefix):\n",
    "        markdown = markdown[len(prefix):]\n",
    "        if markdown.endswith(suffix):\n",
    "            markdown = markdown[:-len(suffix)]\n",
    "\n",
    "    page.markdown = remove_recitation_hack(markdown)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the images so we can render them.\n",
    "def save_markdown_images(page: PDFPage, directory: str=\".\"):\n",
    "    directory = Path(directory)\n",
    "    # Save the images mentioned in the markdown\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    for img in page.extracted_images:\n",
    "        if (img.name in page.markdown):\n",
    "            img.image.save(directory / img.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We saved the images in an output dir, but the image references in Markdown don't have this\n",
    "# prefix. To render markdown in the notebook, I will need to prepend the directory to the image names.\n",
    "\n",
    "def patch_dir_base(input: string, image_names: list[str], prefix: Path):\n",
    "    prefix = Path(prefix)\n",
    "\n",
    "    replacements = { name : str(prefix/name) for name in image_names }\n",
    "    pattern = '|'.join(map(re.escape, replacements.keys()))\n",
    "    return re.sub(pattern, lambda m: replacements[m.group()], input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try with the AIAYN page.\n",
    "if RUN_DEV_CELLS:\n",
    "    page_to_markdown(aiayn.pages[3])\n",
    "    save_markdown_images(aiayn.pages[3], \"aiayn_dev\")\n",
    "    print(aiayn.pages[3].markdown)\n",
    "    display(Markdown(patch_dir_base(aiayn.pages[3].markdown,\n",
    "                                    [image.name for image in aiayn.pages[3].extracted_images],\n",
    "                                    \"aiayn_dev\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looks very good! Let's try with the Lowry page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_DEV_CELLS:\n",
    "    page_to_markdown(lowry.pages[6])\n",
    "    save_markdown_images(lowry.pages[6], \"lowry_dev\")\n",
    "    print(lowry.pages[6].markdown)\n",
    "    display(Markdown(patch_dir_base(lowry.pages[6].markdown,\n",
    "                                    [image.name for image in lowry.pages[6].extracted_images],\n",
    "                                    \"lowry_dev\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It looks decent too, let's tie it all together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's process multiple pages in paralle to make it faster.\n",
    "\n",
    "Note: I'm not sure what's a reasonable number of parallel workers that does not get rate-limited with a free API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "def process_single_page(page, master_pbar):\n",
    "    pbar = tqdm(desc=f\"Page {page.page_num}\", total=3)\n",
    "    pbar.set_postfix_str(\"Checking images...\")\n",
    "    need_extraction = page_needs_image_extraction(page)\n",
    "    pbar.update(1)\n",
    "    if (need_extraction):\n",
    "        pbar.set_postfix_str(\"Generating BBoxes...\")\n",
    "        page_find_bboxes(page)\n",
    "        page_scale_bboxes(page, 1.15)\n",
    "        # bboxes = scale_bboxes(bboxes, 1.15)\n",
    "#         if len(bboxes):\n",
    "#             print(f\"Found bboxes: {bboxes}\")\n",
    "        extract_images_from_page(page)\n",
    "    pbar.update(1)\n",
    "\n",
    "    pbar.set_postfix_str(\"Converting to markdown...\")\n",
    "    page_to_markdown(page)\n",
    "    pbar.update(1)\n",
    "    pbar.close()\n",
    "    master_pbar.update(1)\n",
    "\n",
    "def pdf_to_markdown(pdf: PDFDocument, max_workers=16):\n",
    "    with tqdm(total=len(pdf.pages), desc=\"Processing pages\", unit=\"page\") as pbar:\n",
    "        with ThreadPoolExecutor(max_workers=min(len(pdf.pages), max_workers)) as executor:\n",
    "            process_fn = partial(process_single_page, master_pbar=pbar)\n",
    "            list(executor.map(process_fn, pdf.pages))\n",
    "    pdf.markdown = remove_recitation_hack(\"\\n\\n\".join([page.markdown for page in pdf.pages]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_DEV_CELLS:\n",
    "    pdf_to_markdown(lowry)\n",
    "    for page in lowry.pages:\n",
    "        save_markdown_images(page, \"lowry_dev\")\n",
    "\n",
    "    display(Markdown(patch_dir_base(lowry.markdown,\n",
    "                                    [image.name for page in lowry.pages for image in page.extracted_images],\n",
    "                                    \"lowry_dev\")))\n",
    "\n",
    "    # display(Markdown(lowry.markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's do a second pass.\n",
    "\n",
    "Gemini did a decent job at converting individual pages to PDF, but it made a couple mistakes because for each pages it did not have access to othe pages.\n",
    "I will pass the whole document through Gemini again, to improve consistency between pages, brush up formatting, and optionally will make links to bibliography work properly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we process the entire document, and we need to use a few tricks:\n",
    "- It's possible the whole document will not fit into the output token limit (8K), so we need to generate the output in steps.\n",
    "- Cache the input (whole document + page images + extracted images) to save cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Handle caching, multi-step generation, retries, and errors.\n",
    "\n",
    "# For debugging purposes:\n",
    "\n",
    "multistep_log = []\n",
    "\n",
    "def gemini_generate_multistep(model,\n",
    "                    messages,\n",
    "                    pbar=None,             # If we want to update a progress bar.\n",
    "                    max_total_chars=None,  # We might want to limit the length of generated text to avoid an infinite loop.\n",
    "                    generation_config=None,\n",
    "                    safety_settings=SAFETY_SETTINGS,\n",
    "                    retries=MAX_RETRIES):\n",
    "\n",
    "    attempts = 0\n",
    "    tokens_generated = 0\n",
    "\n",
    "\n",
    "\n",
    "    if pbar: pbar.set_postfix_str(\"Caching input...\")\n",
    "\n",
    "    # print(f\"Caching input: {messages}\")\n",
    "\n",
    "    cache = caching.CachedContent.create(\n",
    "        model=model.model_name,\n",
    "        contents=messages,\n",
    "        ttl=datetime.timedelta(hours=1) # We will delete the cache manually after we are done.\n",
    "    )\n",
    "    cache_model = genai.GenerativeModel.from_cached_content(cache)\n",
    "\n",
    "    multistep_log.append({\"msg\": \"Cachd input\", \"cache\": cache, \"cached_messages\": messages})\n",
    "\n",
    "    # Gemini requires at least one message besides cache. Luckily we always start with a markdown block.\n",
    "    extra = \"```markdown\\n\"\n",
    "\n",
    "    try:\n",
    "        while attempts < retries:\n",
    "            if pbar: pbar.set_postfix_str(\"Generating content...\")\n",
    "            if (attempts > 0):\n",
    "                print(f\"Retry {attempts} of maximum {retries}\")\n",
    "\n",
    "            response = cache_model.generate_content(\n",
    "                [{\"role\": \"model\", \"parts\": [{\"text\": extra}]}],\n",
    "                generation_config=generation_config,\n",
    "                safety_settings=safety_settings\n",
    "            )\n",
    "\n",
    "            if len(response.candidates) == 0:\n",
    "                print(\"No candidates received from the model:\")\n",
    "                print(f\"Input: {messages}\")\n",
    "                print(f\"Extra: `{extra}`\")\n",
    "                print(response)\n",
    "                attempts += 1\n",
    "                continue\n",
    "\n",
    "\n",
    "            if response.candidates[0].finish_reason.name in [\"MAX_TOKENS\", \"STOP\"]:\n",
    "\n",
    "                multistep_log.append({\n",
    "                    \"msg\": \"Response received\",\n",
    "                    \"extra\": [{\"role\": \"model\", \"parts\": [{\"text\": extra}]}],\n",
    "                    \"text\": response.text,\n",
    "                    \"reason\": response.candidates[0].finish_reason.name,\n",
    "                    \"cache\": cache,\n",
    "                    })\n",
    "\n",
    "\n",
    "                if response.candidates[0].finish_reason == Candidate.FinishReason.MAX_TOKENS:\n",
    "                    if pbar:\n",
    "                        pbar.update(len(response.text))\n",
    "                    attempts = 0\n",
    "                    extra += response.text\n",
    "                    # print(response.usage_metadata)\n",
    "                    tokens_generated += response.usage_metadata.candidates_token_count\n",
    "                    if max_total_chars is not None and len(extra + response.text) > max_total_chars:\n",
    "                        print(\"Max total characters reached.\")\n",
    "                        response_text = extra + response.text\n",
    "                        if pbar:\n",
    "                            pbar.set_postfix_str(\"Max total tokens reached.\")\n",
    "                            pbar.refresh()\n",
    "                        return response_text, response.usage_metadata\n",
    "\n",
    "                    continue\n",
    "\n",
    "                if response.candidates[0].finish_reason == Candidate.FinishReason.STOP:\n",
    "                    if pbar:\n",
    "                        pbar.set_postfix_str(\"Finished.\")\n",
    "                        pbar.update(len(response.text))\n",
    "\n",
    "                    multistep_log.append({\n",
    "                        \"msg\": \"Finished\",\n",
    "                        \"extra\": [{\"role\": \"model\", \"parts\": [{\"text\": extra}]}],\n",
    "                        \"text\": response.text,\n",
    "                        \"reason\": response.candidates[0].finish_reason.name,\n",
    "                        \"cache\": cache,\n",
    "                        })\n",
    "\n",
    "                    return extra + response.text, response.usage_metadata\n",
    "\n",
    "\n",
    "            # We can only reach here if the finish reason is not STOP or MAX_TOKENS.\n",
    "            print(f\"Unexpected finish reason {response.candidates[0].finish_reason.name}\")\n",
    "            attempts += 1\n",
    "\n",
    "        if (attempts >= retries):\n",
    "            print(\"Max retries reached\")\n",
    "            print(response)\n",
    "            raise Exception(\"Max retries reached.\")\n",
    "\n",
    "    finally:\n",
    "        if cache: cache.delete()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "brush_it_up_prompt = \"\"\"\n",
    "Brush up formatting in this markdown document. Process the document in full.\n",
    "Do not omit any parts of the document. Do not summarize any parts of the document.\n",
    "Never change the wording or structure of the document, keep content where it is.\n",
    "Remove decorative elements and page sepaeators.\n",
    "\n",
    "If the text has special symbols that would be incorrectly interpreted as markdown formatting, escape them.\n",
    "\n",
    "Keep any images in the document as they are.\n",
    "\"\"\" + common_markdown_prompt\n",
    "\n",
    "\n",
    "def brush_it_up(document: PDFDocument,  pbar):\n",
    "    model = genai.GenerativeModel(model_name=MODEL_NAME)\n",
    "    if not document.markdown:\n",
    "        print(\"The document is missing markdown!\")\n",
    "        return\n",
    "\n",
    "    prompt = brush_it_up_prompt\n",
    "\n",
    "    messages = []\n",
    "    # for page in document.pages:\n",
    "    #     messages.append(f\"Page: {page.page_num}\\n\")\n",
    "    #     messages.append(page.page_image)\n",
    "\n",
    "    messages.append(document.markdown)\n",
    "    messages.append(prompt)\n",
    "\n",
    "\n",
    "    markdown, usage = gemini_generate_multistep(\n",
    "                model,\n",
    "                messages,\n",
    "                # We don't want to create an infinite loop, and double the input size give a good safety margin.\n",
    "                max_total_chars=len(document.markdown) * 1.5,\n",
    "                pbar=pbar,\n",
    "                generation_config=genai.GenerationConfig(\n",
    "                    # I set it to 1000 so we can update the progress bar more often.\n",
    "                    # Don't want to deal with streaming for this purpose.\n",
    "                    max_output_tokens=1000,\n",
    "                    temperature=0,\n",
    "                    top_k=1,\n",
    "                    response_mime_type=\"text/plain\")\n",
    "            )\n",
    "\n",
    "    pbar.set_postfix_str(\"Finished.\")\n",
    "\n",
    "    pbar.refresh()\n",
    "    pbar.close()\n",
    "\n",
    "    prefix = \"```markdown\\n\"\n",
    "    suffix = \"```\"\n",
    "\n",
    "    if markdown.startswith(prefix):\n",
    "        markdown = markdown[len(prefix):]\n",
    "        if markdown.endswith(suffix):\n",
    "            markdown = markdown[:-len(suffix)]\n",
    "\n",
    "\n",
    "    return remove_recitation_hack(markdown), usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_DEV_CELLS:\n",
    "    md, usage = brush_it_up(lowry, tqdm(desc=\"Second pass\", unit=\"character\", total=len(lowry.markdown)))\n",
    "    print(f\"Usage on the last call:\\n{usage}\")\n",
    "    display(Markdown(patch_dir_base(md,\n",
    "                                    [image.name for page in lowry.pages for image in page.extracted_images],\n",
    "                                    \"lowry_dev\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This looks very nice!\n",
    "### Gemini fixed up a few inconsistencies between pages and correctly escaped all markdown syntax in text\n",
    "### Let's tie  it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_markdown(pdf_path: str, output_dir: str= \"output\", second_pass=True):\n",
    "    output_dir = Path(output_dir)\n",
    "\n",
    "    document = extract_pdf_content(pdf_path)\n",
    "    pdf_to_markdown(document)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for page in document.pages:\n",
    "        save_markdown_images(page, output_dir)\n",
    "    if second_pass:\n",
    "        open(output_dir / \"document-preliminary.md\", \"w\").write(document.markdown)\n",
    "        document.markdown_pass1 = document.markdown\n",
    "        document.markdown, usage = brush_it_up(document,  tqdm(desc=\"Second pass\", unit=\"character\", total=len(document.markdown)))\n",
    "        open(output_dir / \"document-final.md\", \"w\").write(document.markdown)\n",
    "        print(f\"Usage on the last call: {usage}\")\n",
    "    else:\n",
    "        open(output_dir / \"document.md\", \"w\").write(document.markdown)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_DEV_CELLS:\n",
    "    multistep_log = []\n",
    "    lowry = convert_pdf_to_markdown(\"Lowry.pdf\", \"lowry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I had a look at the markdown in the \"lowry\" directory, and it looks good. I will now convert the AIAYN PDF.\n",
    "\n",
    "### Now, the competition requires us to use at least 100k context. I will convert the Gemini 1.5 paper, which is 150 pages long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This take a really long time to run.\n",
    "\n",
    "RUN_SUPER_LONG_CONTEXT=True\n",
    "\n",
    "if RUN_DEV_CELLS and RUN_SUPER_LONG_CONTEXT:\n",
    "    convert_pdf_to_markdown(\"gemini-1.5.pdf\", \"gemini1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the final touch - a simple UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import shutil\n",
    "import tempfile\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m show_btn \u001b[38;5;241m=\u001b[39m widgets\u001b[38;5;241m.\u001b[39mButton(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDisplay the document\u001b[39m\u001b[38;5;124m'\u001b[39m, disabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m container \u001b[38;5;241m=\u001b[39m widgets\u001b[38;5;241m.\u001b[39mVBox()\n\u001b[0;32m---> 18\u001b[0m temp_dir \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(tempfile\u001b[38;5;241m.\u001b[39mmkdtemp(\u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mrelative_to(Path\u001b[38;5;241m.\u001b[39mcwd())\n\u001b[1;32m     20\u001b[0m input_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     21\u001b[0m prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "upload = widgets.FileUpload(accept='.pdf', multiple=False)\n",
    "second_pass=widgets.Checkbox(description=\"Two passes (slow but better results)\",\n",
    "                             disabled=True,\n",
    "                             indent=False,\n",
    "                             value=True)\n",
    "parallel=widgets.Checkbox(description=\"Paralel processing (might rate-limit free API keys)\",\n",
    "                             disabled=True,\n",
    "                             indent=False,\n",
    "                             value=True)\n",
    "convert_btn = widgets.Button(description='Convert to Markdown', disabled=True)\n",
    "status = widgets.HTML(value='')\n",
    "download_btn = widgets.Button(description='Download Results', disabled=True)\n",
    "download_link = widgets.HTML()\n",
    "show_btn = widgets.Button(description='Display the document', disabled=True, value=False)\n",
    "\n",
    "container = widgets.VBox()\n",
    "\n",
    "temp_dir = Path(tempfile.mkdtemp(dir=\".\")).relative_to(Path.cwd())\n",
    "\n",
    "input_path = None\n",
    "prefix = None\n",
    "filename = None\n",
    "\n",
    "def on_upload_change(change):\n",
    "    global input_path\n",
    "\n",
    "    global prefix\n",
    "    global filename\n",
    "    if upload.value:\n",
    "        filename = upload.value[0][\"name\"]\n",
    "        file_content = upload.value[0]['content']\n",
    "\n",
    "        # Save uploaded file\n",
    "        input_path = temp_dir/filename\n",
    "        with open(input_path, 'wb') as f:\n",
    "            f.write(file_content)\n",
    "\n",
    "        # Convert\n",
    "        prefix = temp_dir/'output'\n",
    "        os.makedirs(prefix, exist_ok=True)\n",
    "\n",
    "        # Enable conversion when a file is uploaded.\n",
    "        parallel.disabled = False\n",
    "        second_pass.disabled = False\n",
    "        convert_btn.disabled = False\n",
    "\n",
    "\n",
    "zip_filename = None\n",
    "document = None\n",
    "def on_convert_click(b):\n",
    "        global zip_filename\n",
    "        global document\n",
    "\n",
    "        convert_btn.disabled=True\n",
    "        try:\n",
    "            document = convert_pdf_to_markdown(input_path,\n",
    "                                               prefix,\n",
    "                                               second_pass.value,\n",
    "                                               parallel.value)\n",
    "\n",
    "            # Create zip\n",
    "            zip_filename = f'{filename}-output.zip'\n",
    "            zip_path = temp_dir/zip_filename\n",
    "            shutil.make_archive(str(zip_path)[:-4], 'zip', prefix)\n",
    "\n",
    "            # Enable download and show\n",
    "            download_btn.disabled = False\n",
    "            show_btn.disabled = False\n",
    "            status.value = '<span style=\"color: green\">Conversion complete!</span>'\n",
    "        except Exception as e:\n",
    "            download_btn.disabled = True\n",
    "            status.value = f'<span style=\"color: red\">Error: {str(e)}</span>'\n",
    "        convert_btn.disabled=True\n",
    "\n",
    "\n",
    "def on_download_click(b):\n",
    "    with open(os.path.join(temp_dir, zip_filename), 'rb') as f:\n",
    "        content = f.read()\n",
    "        b64 = base64.b64encode(content).decode()\n",
    "    download_link.value = f\"\"\"\n",
    "    <a download=\"{zip_filename}\"\n",
    "       href=\"data:application/zip;base64,{b64}\"\n",
    "       target=\"_blank\">Click to download</a>\n",
    "    \"\"\"\n",
    "\n",
    "markdown_display_handle = None\n",
    "\n",
    "def on_show_click(b):\n",
    "    global markdown_display_handle\n",
    "    display(Markdown(patch_dir_base(document, prefix)), display_id=True)\n",
    "\n",
    "upload.observe(on_upload_change, names='value')\n",
    "convert_btn.on_click(on_convert_click)\n",
    "download_btn.on_click(on_download_click)\n",
    "show_btn.on_click(on_show_click)\n",
    "\n",
    "\n",
    "\n",
    "container.children = [upload, status, second_pass, parallel, convert_btn, download_btn, download_link, show_btn]\n",
    "\n",
    "# Display UI\n",
    "display(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the cell above to reset it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now, one requirement of the challenge was to use at least 100k token context.\n",
    "### The PDFs I've been using for development/demonstration are a bit short for that.\n",
    "### Let's convert the Gemini 1.5 paper. It's 150 pages and will take a long time to process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
